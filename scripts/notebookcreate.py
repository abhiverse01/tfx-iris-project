import json

# Define the notebook content
notebook_content = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFX Iris Pipeline\n",
    "\n",
    "This notebook demonstrates how to create a TFX pipeline using the Iris flower dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Evaluator, Pusher\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.local import local_dag_runner\n",
    "from tfx.orchestration.metadata import sqlite_metadata_connection_config\n",
    "from tfx.proto import example_gen_pb2, trainer_pb2, eval_config_pb2, pusher_pb2\n",
    "\n",
    "def create_pipeline():\n",
    "    data_root = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "    input_config = example_gen_pb2.Input(splits=[\n",
    "        example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n",
    "        example_gen_pb2.Input.Split(name='eval', pattern='eval/*')\n",
    "    ])\n",
    "\n",
    "    example_gen = CsvExampleGen(input_base=data_root, input_config=input_config)\n",
    "    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "    schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema']\n",
    "    )\n",
    "\n",
    "    def preprocessing_fn(inputs):\n",
    "        import tensorflow_transform as tft\n",
    "        outputs = {}\n",
    "        outputs['sepal_length'] = tft.scale_to_z_score(inputs['sepal_length'])\n",
    "        outputs['sepal_width'] = tft.scale_to_z_score(inputs['sepal_width'])\n",
    "        outputs['petal_length'] = tft.scale_to_z_score(inputs['petal_length'])\n",
    "        outputs['petal_width'] = tft.scale_to_z_score(inputs['petal_width'])\n",
    "        outputs['species'] = inputs['species']\n",
    "        return outputs\n",
    "\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        preprocessing_fn=preprocessing_fn\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        module_file=os.path.join(os.getcwd(), 'models', 'model.py'),\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=trainer_pb2.TrainArgs(num_steps=1000),\n",
    "        eval_args=trainer_pb2.EvalArgs(num_steps=100)\n",
    "    )\n",
    "\n",
    "    eval_config = eval_config_pb2.EvalConfig(\n",
    "        slicing_specs=[eval_config_pb2.SlicingSpec()],\n",
    "        metrics_specs=[eval_config_pb2.MetricsSpec(metrics=[\n",
    "            eval_config_pb2.MetricConfig(class_name='ExampleCount'),\n",
    "            eval_config_pb2.MetricConfig(class_name='Accuracy')\n",
    "        ])]\n",
    "    )\n",
    "\n",
    "    evaluator = Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        eval_config=eval_config\n",
    "    )\n",
    "\n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(base_directory='serving_model_dir')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name='iris_pipeline',\n",
    "        pipeline_root='pipeline_output',\n",
    "        components=[\n",
    "            example_gen,\n",
    "            statistics_gen,\n",
    "            schema_gen,\n",
    "            example_validator,\n",
    "            transform,\n",
    "            trainer,\n",
    "            evaluator,\n",
    "            pusher\n",
    "        ],\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=sqlite_metadata_connection_config('metadata.db')\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    local_dag_runner.LocalDagRunner().run(create_pipeline())\n"
   ]
  }
 ]
}

# Write the notebook content to a file
with open('tfx_iris_pipeline.ipynb', 'w') as f:
    json.dump(notebook_content, f)
